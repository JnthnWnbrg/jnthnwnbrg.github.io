<!doctype html>
<html lang="en">
  <head>


    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="description" content="Followeyes : Eyes follow your face" />
    <meta name="keywords" content="Followeyes.js, HTML, CSS, JavaScript, jQuery" />
    <meta name="author" content="Ivan Ryabov" />
    
    <link rel="stylesheet" href="css/followeyes.css">

  </head>

  <body>
    <title>eyes follow u facetracking.html</title>
    <meta http-equiv="X-UA-Compatible" content="IE=Edge"/>
    <meta charset="utf-8">
    <style>
      body {
	  background-color: #f0f0f0;
	  margin-left: 10%;
	  margin-right: 10%;
	  margin-top: 1.5%;
	  /*width: 40%;*/
	  overflow: scroll; /*hidden;*/
	  font-family: Monospace,"Helvetica", Arial, Serif;
	  position: relative;
      }
    </style>
    <script>
      // getUserMedia only works over https in Chrome 47+, so we redirect to https. Also notify user if running from file.
      if (window.location.protocol == "file:") {
	  //zw// alert("You seem to be running this example directly from a file. Note that these examples only work when served from a server or localhost due to canvas cross-domain restrictions.");
      } else if (window.location.hostname !== "localhost" && window.location.protocol !== "https:"){
	  window.location.protocol = "https";
      }
    </script>
    <script type="text/javascript">

      /*
       var _gaq = _gaq || [];
       _gaq.push(['_setAccount', 'UA-32642923-1']);
       _gaq.push(['_trackPageview']);

       (function() {
       var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
       ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
       var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
       })();
      */
      
    </script>
</head>
<body>
  
  <script src="./headtrackr.js"></script>
  
  <!-- 320, 240 -->
  <!-- make eyes closer to webcam for realistic gaze-->
  <!-- <div align="center"><canvas id="eyesC" x="320" y="0" width="320" height="240"></canvas></div> -->
  
  <canvas id="compare" width="320" height="240" style="display:none"></canvas>
  <video id="vid" autoplay loop width="320" height="240"></video>
  <canvas id="overlay" width="320" height="240"></canvas>
  
  <canvas id="debug" width="320" height="240"></canvas>
  <p>Use webcam device.</br></p>

  <p id='gUMMessage'></p>
  <p>Status : <span id='headtrackerMessage'></span>
  <input type="button" onclick="htracker.stop();htracker.start();" value="reinitiate facedetection"></input>
    </p>
  <input type="checkbox" onclick="showProbabilityCanvas()" value="asdfasd"></input>-Show probability-map   |
  <input type="checkbox" onclick="showOverlayCanvas()" value="asdfasd"></input>-Show/hide overlay   |
  <input type="checkbox" onclick="toggleMirror()" value="false"></input>-Show/hide mirror </br>
  <input type="checkbox" onclick="toggleVR()" value="false"></input>-toggle VR (stereoscopic) mode
  <!-- indentation in emacs buggy if write <- arrow ... and "false" doesn't do anything -->
					 
<div align="center"><canvas id="eyesC" x="320" y="0" width="640" height="240"></canvas></div>
Try moving back initially. Robot has depth perception and eyes dilate with brightness changes (though the webcam aperture adapts eventually). Try covering the webcam or shining a flashlight or moving up, down, side to side, back and forth.
<script>
  
  
  // set up video and canvas elements needed
  
  var videoInput = document.getElementById('vid');
  var canvasInput = document.getElementById('compare');
  var canvasOverlay = document.getElementById('overlay');

  var canvasEyes = document.getElementById('eyesC');
  var debugOverlay = document.getElementById('debug');
  var overlayContext = canvasOverlay.getContext('2d');
  var eyesContext = canvasEyes.getContext('2d');
  
  canvasOverlay.style.position = "absolute";
  canvasOverlay.style.top = '0px';
  canvasOverlay.style.zIndex = '100001';
  canvasOverlay.style.display = 'block';
  //debugOverlay.style.position = "absolute";
  debugOverlay.style.top = '0px';
  debugOverlay.style.zIndex = '100002';
  debugOverlay.style.display = 'none';
  
  // add some custom messaging
  
  statusMessages = {
      "whitebalance" : "checking for stability of camera whitebalance",
      "detecting" : "Detecting face",
      "hints" : "Hmm. Detecting the face is taking a long time. Try moving back.",//. Try better lighting",
      "redetecting" : "Lost track of face, redetecting",
      "lost" : "Lost track of face",
      "found" : "Tracking face"
  };
  
  supportMessages = {
      "no getUserMedia" : "Unfortunately, <a href='http://dev.w3.org/2011/webrtc/editor/getusermedia.html'>getUserMedia</a> is not supported in your browser. You could try downloading "
      //+"<a href='http://www.opera.com/browser/'>Opera 12</a> or "
      +"<a href='http://caniuse.com/stream'>another browser that supports getUserMedia</a>. But don't worry, now using fallback video for facedetection.",
      "no camera" : "No camera/webcam found. Using fallback video for facedetection."
  }; //opera 12 doesn't fix it
  
  document.addEventListener("headtrackrStatus", function(event) {
      if (event.status in supportMessages) {
	  var messagep = document.getElementById('gUMMessage');
	  messagep.innerHTML = supportMessages[event.status];
      } else if (event.status in statusMessages) {
	  var messagep = document.getElementById('headtrackerMessage');
	  messagep.innerHTML = statusMessages[event.status];
      }
  }, true);
  
  // the face tracking setup
  
  var htracker = new headtrackr.Tracker({altVideo : {ogv : "./media/capture5.ogv", mp4 : "./media/capture5.mp4"}, calcAngles : true, ui : false, headPosition : false, debug : debugOverlay});
  htracker.init(videoInput, canvasInput);
  htracker.start();

  // https://stackoverflow.com/questions/13762864/image-dark-light-detection-client-sided-script
  function getLightness() {
      // get canvas context
      var canvas = document.createElement("canvas");//getElementById('canvas');
      //webcam canvas from headtrackr.js
      canvas.width=videoInput.width;
      canvas.height=videoInput.height;
      var ctx = canvas.getContext("2d");

      //must copy, as a comment in headtrackr.js says
      ctx.drawImage(videoInput,0,0,videoInput.width,videoInput.height);
      // https://developer.mozilla.org/en-US/docs/Web/API/Canvas_API/Tutorial/Pixel_manipulation_with_canvas
      //console.log(ctx);
      var imageData = ctx.getImageData(0,0,canvas.width,canvas.height);
      var data = imageData.data;
      //console.log("imageData"+imageData);
      var r,g,b,avg
      var colorSum=0;

      for(var x = 0, len = data.length; x < len; x+=4) {
          r = data[x];
          g = data[x+1];
          b = data[x+2];

          avg = Math.floor((r+g+b)/3);
          colorSum += avg;
      }
      //console.log("colorSum"+colorSum);
      var mybrightness = Math.floor(colorSum / (canvas.width*canvas.height));
      //console.log("bright "+mybrightness);
      return mybrightness; 
  }
  
  var robotFaceSize = 120;
  var hairLength=robotFaceSize-10+40; //long hair
  var skin= "#EE9955";//peach //"#662200";//brown //"#331100"; //African
  var hair="#030100"; //dark black brown
  var shirt="#880088";
  
  var r = 20; //radius //eyes radius
  var defaultGap = r*3/2; // r; //cringe eyes
  var gap=defaultGap;
  var gapNow = gap;
  var blinking=false;
  var sleeping=false;
  var shouldSleep=true;
  var cursorX=overlay.width/2;//default... not robotX
  var cursorY=overlay.height/2;//default
  var closeness=(overlay.width/3)*overlay.height;//default
  var pupilX,pupilY;
  var pupilsX=[4,6]; //bogus values, used for debugging depth perception
  //pupilsX[1]-pupilsX[0]

  var brightness=0;//zw
  
  var irisSize=r*1.1*1.6;//idk, 1.7? not 2r
  var minPupilSize= irisSize/1.9;//r*1.1;//1.8;//*1.2;
  var maxPupilSize=irisSize/1.4;
  
  // for each facetracking event received draw rectangle around tracked face on canvas
  
  document.addEventListener("facetrackingEvent", function( event ) {
      // clear canvas
      overlayContext.clearRect(0,0,overlay.width,overlay.height);//320,240);
      //overlayContext.strokeRect(0,0,overlay.width,overlay.height);//320,240);
      
      // once we have stable tracking, draw rectangle
      if (event.detection == "CS") {

	  overlayContext.translate(event.x, event.y)
	  overlayContext.rotate(event.angle-(Math.PI/2));
	  overlayContext.strokeStyle = "#00CC00";
	  overlayContext.strokeRect((-(event.width/2)) >> 0, (-(event.height/2)) >> 0, event.width, event.height);
	  overlayContext.rotate((Math.PI/2)-event.angle);
	  overlayContext.translate(-event.x,  -event.y);



	  ///////start zw/////
	  cursorX = overlay.width-event.x;//because video input is mirrored 
          cursorY = event.y; //eyes used to follow cursor, now face
	  closeness=event.width*event.height;//Math.max(event.width,event.height); //depth perception
	  //large face means closer
          /////end////
      }
  }); //end event listener

  function drawIrisOrPupil(eyeCenterX,eyeCenterY,pupilSize) { //pupil or iris size
      //if pupil is big and appears out of boundary of eye whites
      var leftEdgeWarp=Math.max(0,(eyeCenterX-r)-(pupilX-pupilSize/2));
      var topEdgeWarp=Math.max(0,(eyeCenterY-r)-(pupilY-pupilSize/2));
      var rightEdgeWarp=Math.max(0,-(eyeCenterX+r)+(pupilX+pupilSize/2));
      var bottomEdgeWarp=Math.max(0,-(eyeCenterY+r)+(pupilY+pupilSize/2));

      eyesContext.fillRect((pupilX-pupilSize/2)+leftEdgeWarp, (pupilY-pupilSize/2)+topEdgeWarp, pupilSize-leftEdgeWarp-rightEdgeWarp, pupilSize-topEdgeWarp-bottomEdgeWarp); //PEMDAS
  }

  var twins=false;
  var simulatedClosenessEnabled=true;
  function vrSide(robotX){
      var eyeCenterX; //default
      var eyeCenterY = overlay.height/2;
      var eyeSpacing=60;
      var eyes = [robotX-eyeSpacing/2, robotX+eyeSpacing/2];
      var realEyes;
      
      if(twins) //2 robots
	  realEyes=eyes;
      else
	  realEyes = [eyesC.width/2-eyeSpacing/2, eyesC.width/2+eyeSpacing/2];
      eyesContext.fillStyle=hair;
      var hairWidth=20;
      eyesContext.fillRect(robotX-robotFaceSize/2-hairWidth/2, eyeCenterY-40-20, robotFaceSize+hairWidth, hairLength);

      
      //shirt
      eyesContext.fillStyle = shirt;
      eyesContext.fillRect(robotX-robotFaceSize/2-30, robotFaceSize+90, robotFaceSize+60, robotFaceSize+10+40);

      
      //neck
      eyesContext.fillStyle = skin;
      var neckWidth=robotFaceSize/3;
      eyesContext.fillRect(robotX-neckWidth/2, eyeCenterY, neckWidth, robotFaceSize-10);

      
      //face
      eyesContext.fillStyle = skin;
      eyesContext.fillRect(robotX-robotFaceSize/2, eyeCenterY-30, robotFaceSize, robotFaceSize-20);
      eyesContext.strokeStyle = hair;//hair color for shading // "#050505";//grey 
      eyesContext.strokeRect(robotX-robotFaceSize/2, eyeCenterY-30, robotFaceSize, robotFaceSize-20);//chin, face outline 
      //male is square, female horizontal small forehead
      
      var mouthSize = 30;//50; //wider is masculine
      eyesContext.fillStyle = "#A97300"; 
      
      var upperLipY=eyeCenterY+40-4;
      
      eyesContext.fillRect(robotX-mouthSize/2, upperLipY, mouthSize, 8); //upper lip
      
      eyesContext.fillStyle = "#A96300";//"#AF9900"; //yellow //"#090300"; //black
      //"#050100" ; //black // "#AA0000"; //red lips               
      eyesContext.fillRect(robotX-mouthSize/2, upperLipY+8, mouthSize, 8);

      eyesContext.fillStyle = "#000000"; 
      eyesContext.fillRect(robotX-mouthSize/2, upperLipY+7, mouthSize, 1); //lip gap outline

      //var blink =false;
      if(Math.random()>0.99){
	  blinking=true;
      }

      var sleepiness = 1/3;
      var wakeChance=1-sleepiness;
      if(shouldSleep && Math.random()>0.9993){ //close eyes
	  sleeping=true;
	  gap=1;
      } else {
	  if(!shouldSleep || Math.random()>0.9950){ //wake
	      sleeping=false;
	      gap=defaultGap;
	  }
	  
      }

      var blinkSpeed=8*60/fps;
      if(blinking && gapNow>1){
	  gapNow=Math.max(gapNow-blinkSpeed,1);
	  // gapNow=1; //1;//1 pixel
	  
      } else {
	  blinking=false;
	  gapNow=Math.min(gapNow+blinkSpeed,gap);
      }

      var simulatedCloseness;
      if(simulatedClosenessEnabled && vrMode && !twins){
	  simulatedCloseness=closeness/2;
      }else simulatedCloseness=closeness;
      
      brightness = getLightness();
      //console.log("brightness "+brightness);
      for (var i = eyes.length - 1; i >= 0; i--) {
	  eyeCenterX=eyes[i]; //undefined error sometimes during code changes

	  var gazeCorrectionY = -50; //because webcam is above screen
	  
	  pupilX = eyeCenterX-(i-0.5)*0.05*simulatedCloseness/overlay.width+(r*(cursorX*eyesC.width/overlay.width-realEyes[i])/overlay.width);//*(Math.pow(0.4,closeness/overlay.width)));///overlay.width);
	  //event.width is for depth perception, larger face is closer 
	  pupilY = eyeCenterY+(r*(cursorY+gazeCorrectionY-eyeCenterY)/overlay.height);
	  pupilsX[i]=pupilX; //for debugging

	  //center
	  eyesContext.strokeRect(eyeCenterX-5, eyeCenterY-5, 10, 10);
	  

	  //whites
	  eyesContext.fillStyle = "#FFFFFF";     
	  eyesContext.strokeStyle = "black";
	  //if non-Asian then outline eyes
	  //eyesContext.strokeRect(eyeCenterX-r, eyeCenterY-r, r*2, r*2);
	  
	  eyesContext.fillRect(eyeCenterX-r, eyeCenterY-r, r*2, r*2);
	  
	  //iris
	  eyesContext.fillStyle = "#663300";//brown "#6A6A00";//green "#6A6AFF";//blue
	  drawIrisOrPupil(eyeCenterX,eyeCenterY,irisSize);//6);

	  //pupil
	  eyesContext.fillStyle = "#000000";//black     
	  eyesContext.strokeStyle = "#FF00FF";            

	  drawIrisOrPupil(eyeCenterX,eyeCenterY,minPupilSize+(maxPupilSize-minPupilSize)*(255-brightness)/255);
	  //0-255 bright

	  //eyelid outlines
	  eyesContext.strokeStyle = hair; //change later for bleached hair
	  
	  
	  
	  eyesContext.fillStyle=hair;//color
	  //with eyeliner
	  //eyesContext.fillRect(eyeCenterX-r-1, eyeCenterY-r-1, r*2+2, r-gapNow/2+4); //outline eyelid
	  //without eyeliner, just upper eyelashes
	  eyesContext.fillRect(eyeCenterX-r, eyeCenterY-gapNow/2-1, r*2, 4); //outline eyelid gap
	  //prevents eyes from appearing like they follow while closed

	  eyesContext.fillStyle=skin; //eyelids
	  eyesContext.fillRect(eyeCenterX-r, eyeCenterY-r, r*2, r-gapNow/2);
	  eyesContext.fillRect(eyeCenterX-r, eyeCenterY+gapNow/2, r*2, r-gapNow/2);

      }
      //console.log("eyeCenterX ", eyeCenterX);
  }

  var vrMode=false;
  var fps=20;//60; //20 for debugging so as not to lag if stuck
  function drawEyes() {
      eyesContext.clearRect(0,0,eyesC.width,eyesC.height);//320,240);
      eyesContext.strokeRect(0,0,eyesC.width,eyesC.height);//320,240);

      if(vrMode){
	  vrSide(eyesC.width/2 -100);
	  vrSide(eyesC.width/2 +100);
      }else{
	  vrSide(eyesC.width/2);
      }
      
  }

  
  function toggleVR() {
      if (vrMode) {
	  vrMode=false;
      } else {
	  vrMode=true;
      }
  }
  
  function toggleMirror() {
      //var inputVideoCanvas = document.getElementById('vid');
      if (videoInput.style.display == 'none') {
	  videoInput.style.display = 'block';
      } else {
	  videoInput.style.display = 'none';
      }
  }
  
  function showOverlayCanvas() {
      //var overlayCanvas = document.getElementById('overlay');
      if (canvasOverlay.style.display == 'none') {
	  canvasOverlay.style.display = 'block';
      } else {
	  canvasOverlay.style.display = 'none';
      }
  }

  // turn off or on the canvas showing probability
  function showProbabilityCanvas() {
      var debugCanvas = document.getElementById('debug');
      if (debugCanvas.style.display == 'none') {
	  debugCanvas.style.display = 'block';
      } else {
	  debugCanvas.style.display = 'none';
      }
  }

  setInterval(drawEyes, 1000/fps);
  </script>
</body>
</html>
